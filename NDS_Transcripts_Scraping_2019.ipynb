{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plackie/planets/blob/master/NDS_Transcripts_Scraping_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVsRJvfUD-yr"
      },
      "source": [
        "## Documentation and usage\n",
        "\n",
        "This Colab notebook was written by **Hiromichi Ueda** '21 (DataSquad) in February 2021.\n",
        "\n",
        "Revised by:\n",
        "* **Helen Du** '22 (DataSquad) **Isabella Cha** '23 (DataSquad) in June 2021\n",
        "* **Cathy Duan** '25 (DataSquad), **Graham Gordon** '25 (DataSquad), **Charles Nykamp** '25 (DataSquad), and **Nina Sun** '23 (DataSquad) in October 2022\n",
        "\n",
        "**Aaron Bronstone** '24 and **Serafin Patino** '24 (DataSquad)\n",
        "\n",
        "Last execution in October 2022 on Google Colab.\n",
        "\n",
        "This script takes an .xls file from NDS and gives a .txt file of the news broadcast transcripts.\n",
        "\n",
        "For basic usage of how to run colab, go to [official intro](https://colab.research.google.com/notebooks/intro.ipynb#recent=true)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6D1ufT43Tco"
      },
      "source": [
        "# Install required modules and mount drive data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKSVv0pHSyui",
        "outputId": "35116565-4dc6-4534-b734-9698ced741e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org (108.157.162.103)] [Connecting to ppa.la\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [2 InRelease 15.6 kB/119 kB 13%] [3 InRelease 14.2 kB/110 kB 13%] [Connected to cloud.r-project.o\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 72.1 kB/119 kB 61%] [3 InRelease 67.7 kB/110 kB 61%] [Connected to ppa.launchpadcont\r                                                                                                    \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,194 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,420 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,274 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,466 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 5,696 kB in 2s (3,173 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 26.4 MB of archives.\n",
            "After this operation, 116 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.2 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.11 [78.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.11 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.58+22.04.1 [23.8 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.3 [2,908 B]\n",
            "Fetched 26.4 MB in 1s (26.4 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.2_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.2) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.11) over (249.11-0ubuntu3.9) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 121081 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.58+22.04.1_amd64.deb ...\n",
            "Unpacking snapd (2.58+22.04.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.2) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.11) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.58+22.04.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.aa-prompt-listener.service → /lib/systemd/system/snapd.aa-prompt-listener.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 121314 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.3_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.3) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.3) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.11) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.15.2-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.23.1-py3-none-any.whl (448 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.3/448.3 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.15.2 trio-0.23.1 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting Selenium-Screenshot\n",
            "  Downloading Selenium-Screenshot-2.1.0.tar.gz (5.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from Selenium-Screenshot) (9.4.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (from Selenium-Screenshot) (4.15.2)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium->Selenium-Screenshot) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium->Selenium-Screenshot) (0.23.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium->Selenium-Screenshot) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium->Selenium-Screenshot) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->Selenium-Screenshot) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->Selenium-Screenshot) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->Selenium-Screenshot) (3.4)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->Selenium-Screenshot) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->Selenium-Screenshot) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->Selenium-Screenshot) (1.1.3)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium->Selenium-Screenshot) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->Selenium-Screenshot) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->Selenium-Screenshot) (0.14.0)\n",
            "Building wheels for collected packages: Selenium-Screenshot\n",
            "  Building wheel for Selenium-Screenshot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Selenium-Screenshot: filename=Selenium_Screenshot-2.1.0-py3-none-any.whl size=6452 sha256=954fa030aaa55af4a2b019f48271d1d1637a2899f77e08b4e8a7c76104aa9e84\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/23/da/9608e92dd89d7ca93fead200157924475f58895dc20a1e3654\n",
            "Successfully built Selenium-Screenshot\n",
            "Installing collected packages: Selenium-Screenshot\n",
            "Successfully installed Selenium-Screenshot-2.1.0\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "!pip install Pillow\n",
        "!pip install Selenium-Screenshot\n",
        "!pip install colorama\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-cdQ_GYnZ7K"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import pathlib\n",
        "from itertools import product\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, UnexpectedAlertPresentException, ElementClickInterceptedException\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "\n",
        "# UPDATE 10/22/23\n",
        "from PIL import Image\n",
        "from Screenshot import Screenshot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJzo4Orq3oNh",
        "outputId": "5c8b6567-c951-4017-9019-756ee049fe3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n",
            "/content/drive/Shareddrives/Election Local News Project/NDS News Scraping [F23] 2019/News Transcripts\n"
          ]
        }
      ],
      "source": [
        "# mount Drive\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/Shareddrives/'Election Local News Project'/'NDS News Scraping [F23] 2019'/'News Transcripts'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uCmJWgW35LW"
      },
      "outputs": [],
      "source": [
        "# change this line to navigate to the desired location (optional)\n",
        "# %cd /content/drive/Shareddrives/\"Election Local News Project\"/\"Don't Touch: Floyd Protest, Chauvin Trial News Coverage MEDIA (video & text)\"/\"Chauvin Trial News Coverage\"/\"Chauvin Trial Transcripts\"\n",
        "# %cd \"/content/drive/My Drive/LOCAL DATASQUAD/News Transcripts\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp1u6cNz4mfv"
      },
      "source": [
        "# Body of code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4yssfELdT2p"
      },
      "source": [
        "## unique_program.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BW0bX3CfeIm"
      },
      "outputs": [],
      "source": [
        "# Given a xls file (nds_xls) downloaded from News Data Service, returns all the unique broadcasts in the xls as csv (programs_file).\n",
        "# Last execution: Sep 2021, Google Colab\n",
        "\n",
        "# convert \"8:00 AM CT\" to \"08:00AM CT\"\n",
        "def format_time(time_str):\n",
        "    if time_str[1] == ':':\n",
        "        return_str = '0' + time_str\n",
        "    else:\n",
        "        return_str = time_str\n",
        "    # convert \"08:00 AM CT\" to \"08:00AM CT\"\n",
        "    return return_str[:5] + return_str[6:]\n",
        "\n",
        "def get_unique_program(nds_xls, programs_file):\n",
        "    programs = pd.read_excel(nds_xls)\n",
        "    # the five columns together serve as a unique identifier for each program\n",
        "    program_uni = programs.loc[:, ['Date', 'Time', 'Title', 'Source', 'Market']].drop_duplicates()\n",
        "    # date from Jul 5 2020 -> 2020-07-05\n",
        "    df = program_uni.assign(Date = pd.to_datetime(program_uni['Date'], format='%b %d %Y')).astype(str)\n",
        "    df['Time'] = df['Time'].apply(format_time) # make time conversion\n",
        "    df['Scraped'] = False # initialize 'Scraped' column to false\n",
        "    df_sorted = df.sort_values(['Market', 'Source', 'Date', 'Time'])\n",
        "    df_sorted.reset_index(drop=True, inplace=True)\n",
        "    df_sorted.to_csv(programs_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCyRTdRXOT_D"
      },
      "source": [
        "**9/4 by Helen: I added the \"Scraped\" column to programs_file in the get_unique_program() function. Before that I ran into an error that says programs_file does not have a \"Scraped\" column... I thought Hiro added that column to programs_file in nds_crawler() since that's what he said on the README, but I checked the code and it really did not add anything to programs_file. Maybe Hiro meant to say urls_file but he typed programs_file instead, on the README.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfHBKdRcdoHx"
      },
      "source": [
        "## nds_crawler.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfkZpBcy90hu"
      },
      "source": [
        "For each unique combination of Date, Source, Market columns in programs_file csv,\n",
        "make a query and write the results to csv (urls_file).\n",
        "Last execution: Sep 2021, in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st3xHdVfkUJR"
      },
      "source": [
        "**6/25 by Helen: While I was checking the XPath, I found something weird: for national stations, there are two possible options to choose from in the States box and the Cities box: \"National News Networks\" and \"National Networks (All Other)\". The national stations we want (ABC, CBS, CNN, FOX, FOXNEWS, MSNBC, NBC) can be found under both options, except for PBS, which can only be found under \"National Networks (All Other)\". However, for the 7 stations that can be searched using both options, the search results are different... I have only tried to compare the search results for ABC using the two different options, and it seems like the search results under \"National Networks (All Other)\" is a subset of the search results under \"National News Networks\". But I'm not sure if it is the same situation for other stations. For now, I think we should use \"National News Networks\" (in both the States and Cities boxes) for all national stations except for PBS. We can try this first, and if there are failed queries, we can deal with those later...**\n",
        "\n",
        "**10/23/23 by Aaron: CCX was missing from the old hard coded \"station_to_num\" dictionary, which caused some indexing issues and improper scraping of local news sources. We added CCX in at position 2.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ys18PL0c0G9"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZj93KZ7frLL"
      },
      "outputs": [],
      "source": [
        "# This function takes in the name of the market as input, and returns the x_path of the state and city as output.\n",
        "# remember to go to NDS to check whether the x-path is right or not, since NDS may change the order of the Market name.\n",
        "# To be more specific, you should check, for example, the number (5, in this case) in \"//*[@id=\"states_listbox\"]/li[5].\n",
        "# NOTE: xpath indexes start at 1, not 0. So the number in li[] should be the data-offset-index plus 1\n",
        "def return_Market_xpath(Market):\n",
        "    if Market == 'Minneapolis-St. Paul, MN':\n",
        "        return ['//*[@id=\"states_listbox\"]/li[5]', '//*[@id=\"cities_listbox\"]/li[3]']\n",
        "\n",
        "    elif Market == 'Madison, WI':\n",
        "        return ['//*[@id=\"states_listbox\"]/li[7]', '//*[@id=\"cities_listbox\"]/li[4]']\n",
        "\n",
        "    elif Market == 'Cedar Rapids-Waterloo-Dubuque, IA':\n",
        "        return ['//*[@id=\"states_listbox\"]/li[4]', '//*[@id=\"cities_listbox\"]/li[1]']\n",
        "\n",
        "    elif Market == 'National News Networks':\n",
        "        # note: National News Networks does not contain PBS\n",
        "        return ['//*[@id=\"states_listbox\"]/li[1]', '//*[@id=\"cities_listbox\"]/li[1]']\n",
        "\n",
        "    elif Market == 'National Networks (All Other)':\n",
        "        # for PBS\n",
        "        return ['//*[@id=\"states_listbox\"]/li[2]', '//*[@id=\"cities_listbox\"]/li[1]']\n",
        "\n",
        "    else:\n",
        "        raise Exception(f\"Market name {Market} is not valid\")\n",
        "\n",
        "\n",
        "# This function takes in the name of the station as input, and returns the x_path of the station as output.\n",
        "# remember to go to NDS to check whether the x-path is right or not, since NDS may change the order of the source name.\n",
        "# for all states/cities except for National Networks (All Other)\n",
        "def return_Source_xpath_local(Source):\n",
        "\n",
        "    station_to_num = {\n",
        "\n",
        "        # Local news stations, with their location on the\n",
        "        # drop down menu\n",
        "        'CBSN Minneapolis': '1',\n",
        "        'CCX' : '2',\n",
        "        'KARE': '3',\n",
        "        'KCCO-AM': '4',\n",
        "        'KDWB-FM': '5',\n",
        "        'KEEY-FM': '6',\n",
        "        'KFAN-FM': '7',\n",
        "        'KFXN-FM': '8',\n",
        "        'KLTF-AM': '9',\n",
        "        'KMSP': '10',\n",
        "        'KNOW-FM': '11',\n",
        "        'KNSI-AM': '12',\n",
        "        'KOWZ-FM': '13',\n",
        "        'KQQL-FM': '14',\n",
        "        'KQRS-FM': '15',\n",
        "        'KSTC': '16',\n",
        "        'KSTP': '17',\n",
        "        'KSTP-AM': '18',\n",
        "        'KTCA': '19',\n",
        "        'KTCI': '20',\n",
        "        'KTCZ-FM': '21',\n",
        "        'KTLK-AM': '22',\n",
        "        'KTMY-FM': '23',\n",
        "        'KTWN-FM': '24',\n",
        "        'TPT': '25',\n",
        "        'WCCO': '26',\n",
        "        'WCCO-AM': '27',\n",
        "        'WFTC': '28',\n",
        "        'WMNN-AM': '29',\n",
        "        'WUCW': '30',\n",
        "        'WUMN': '31',\n",
        "        'WVAL-AM': '32',\n",
        "    }\n",
        "\n",
        "    return '//*[@id=\"sources_listbox\"]/li[' + station_to_num[Source] + ']'\n",
        "\n",
        "\n",
        "\n",
        "def return_Source_xpath_national_all_other(source):\n",
        "    station_to_num = {\n",
        "\n",
        "        # National news (All Other) stations, with their location on the\n",
        "        # drop down menu\n",
        "        'ABC' : '3',\n",
        "        'ABC News': '4',\n",
        "        'CBS' : '43',\n",
        "        'CBS News': '44',\n",
        "        'CNN' : '62',\n",
        "        'FOX' : '127',\n",
        "        'FOXNEWS' : '134',\n",
        "        'MSNBC' : '200',\n",
        "        'NBC' : '211',\n",
        "        'NBC News Now': '212',\n",
        "        'PBS': '239',\n",
        "\n",
        "    }\n",
        "\n",
        "    return '//*[@id=\"sources_listbox\"]/li[' + station_to_num[source] + ']'\n",
        "    # return '//*[@id=\"sources_listbox\"]/li[text()=' + Source + ']'\n",
        "\n",
        "\n",
        "def return_Source_xpath_national_news_networks(source):\n",
        "    station_to_num = {\n",
        "        'ABC': '1',\n",
        "        'CBS': '5',\n",
        "        'CNN': '7',\n",
        "        'FOX' : '14',\n",
        "        'FOXNEWS': '15',\n",
        "        'MSNBC': '17',\n",
        "        'NBC': '18',\n",
        "        # No PBS in National News Networks - it's only in all-other\n",
        "    }\n",
        "    return f'//*[@id=\"sources_listbox\"]/li[{ station_to_num[source] }]'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHQCG8A9f_UU"
      },
      "outputs": [],
      "source": [
        "def nds_crawl(username: 1,password: 1,programs_file: 1,urls_file,urls_file_100, crawled_file, failed_query_file, driver_option):\n",
        "    Programs_List = pd.read_csv(programs_file)\n",
        "    # The three columns identify exactly what is needed to make a query in NDS\n",
        "    Query_List = Programs_List.loc[:, ['Date', 'Source', 'Market']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # If urls_file already exists, read it and delete the queries that are already done from Query_List\n",
        "    # BUG: if urls_file contains a query that Query_List does not, that new query gets added\n",
        "    # if pathlib.Path(urls_file).is_file():\n",
        "    #   df_urls = pd.read_csv(urls_file)\n",
        "    #   done_Query_List = df_urls.loc[:, ['Date', 'Source', 'Market']].drop_duplicates().reset_index(drop=True)\n",
        "    #   Query_List = pd.concat([Query_List, done_Query_List]).drop_duplicates(keep=False).reset_index(drop=True)\n",
        "    #   print(\"Query list\")\n",
        "    #   print(Query_List)\n",
        "\n",
        "    print(Query_List)\n",
        "\n",
        "    links = [] # the list to store all the URLs\n",
        "    successful_query = []\n",
        "    failed_query = []\n",
        "    query_idx = 0\n",
        "    num_query = Query_List.shape[0]\n",
        "    print(\"Making {} total queries\".format(num_query))\n",
        "\n",
        "    while query_idx < num_query: # query is not completed\n",
        "        Driver_Success = True # Driver has not encounterd a fatal failure\n",
        "\n",
        "        # (re)start the webdriver in background\n",
        "        driver = webdriver.Chrome(options=driver_option)\n",
        "        # login\n",
        "        driver.get('https://portal.newsdataservice.com/ProgramList')\n",
        "\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.ID, \"Loginform\"))\n",
        "        )\n",
        "\n",
        "        driver.find_element(by=By.XPATH, value='//*[@id=\"Loginform\"]/div/div[2]/div/p[3]/input').click()\n",
        "        driver.find_element(by=By.XPATH, value='//*[@id=\"Loginform\"]/div/div[2]/div/p[3]/input').send_keys(username)\n",
        "        driver.find_element(by=By.XPATH, value='//*[@id=\"Loginform\"]/div/div[2]/div/p[5]/input').click()\n",
        "        driver.find_element(by=By.XPATH, value='//*[@id=\"Loginform\"]/div/div[2]/div/p[5]/input').send_keys(password)\n",
        "        driver.find_element(by=By.XPATH, value='//*[@id=\"submitBtn\"]').click()\n",
        "        print(\"Logging in to new NDS instance...\")\n",
        "\n",
        "        # continue with the same driver while a query remains to be conducted\n",
        "        # and the driver has not encounterd any fatal failure\n",
        "        first_query = True\n",
        "        while Driver_Success and (query_idx < num_query):\n",
        "            row = Query_List.loc[query_idx,]\n",
        "            Market = row['Market']\n",
        "            Source = row['Source']\n",
        "            Date_str = row['Date']\n",
        "            # a list to store a program by its identification and its url\n",
        "            programs = []\n",
        "            print(Fore.BLACK)\n",
        "            print(Back.GREEN)\n",
        "            print(f\"=== Query {query_idx}, which is for market {Market}, source {Source}, and date {Date_str} ===\")\n",
        "            print(Style.RESET_ALL)\n",
        "\n",
        "            # defining fields to select\n",
        "            Market_x_path = return_Market_xpath(Market)\n",
        "            State_x_path = Market_x_path[0]\n",
        "            City_x_path = Market_x_path[1]\n",
        "\n",
        "            Source_x_path = ''\n",
        "            if Market == 'National News Networks':\n",
        "                Source_x_path = return_Source_xpath_national_news_networks(Source)\n",
        "            elif Market == 'National Networks (All Other)':\n",
        "                Source_x_path = return_Source_xpath_national_all_other(Source)\n",
        "            else:\n",
        "                Source_x_path = return_Source_xpath_local(Source)\n",
        "\n",
        "            # Source_x_path = f\"*[@id='sources_listbox']/li[. = '{Source}']\"\n",
        "\n",
        "            num_failed_attempt = 0\n",
        "            Submit_Timeout = False\n",
        "            Query_Success = False # status of most recent attempt of this particular query\n",
        "            # until we succeed or make 3 failed attepts\n",
        "            Added_to_Failed = False\n",
        "\n",
        "            while (not Submit_Timeout) and (not Query_Success) and (num_failed_attempt < 3):\n",
        "                # (re)start the attempt\n",
        "\n",
        "                try:\n",
        "                    if first_query:\n",
        "                      # navigating to search\n",
        "                      print(\"Finding the Broadcast Content tab\")\n",
        "                      nav_button = WebDriverWait(driver, 20).until(\n",
        "                          EC.element_to_be_clickable((By.XPATH, '//*[@id=\"navigation\"]/li[6]/a'))\n",
        "                      )\n",
        "                      #time.sleep(7)\n",
        "                      #By.XPATH, '/html/div[2]'\n",
        "                      WebDriverWait(driver, 10).until(\n",
        "                          EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                      )\n",
        "                      time.sleep(1)\n",
        "                      first_query = False\n",
        "                      print('Clicking on the Broadcast Content tab')\n",
        "                      nav_button.click()\n",
        "                      #time.sleep(7)\n",
        "                      WebDriverWait(driver, 10).until(\n",
        "                          EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                      )\n",
        "                    else:\n",
        "                      print(\"Clearing search boxes\")\n",
        "                      WebDriverWait(driver, 10).until(\n",
        "                          EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                      )\n",
        "\n",
        "                      WebDriverWait(driver, 30).until(\n",
        "                          EC.element_to_be_clickable((By.XPATH, '//*[@id=\"clearAllStates\"]'))).click()\n",
        "                      WebDriverWait(driver, 10).until(\n",
        "                            EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                        )\n",
        "\n",
        "                    # clicking on states box\n",
        "                    print(\"Finding states box\")\n",
        "                    states_box = WebDriverWait(driver, 20).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"states_taglist\"]'))\n",
        "                    )\n",
        "                    print(\"Clicking states box\")\n",
        "                    states_box.click()\n",
        "\n",
        "                    #time.sleep(7)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "                    # selecting state\n",
        "                    print(\"Selecting the state (or national)\")\n",
        "                    state_selection = WebDriverWait(driver, 20).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, State_x_path))\n",
        "                    )\n",
        "                    print('Clicking the state selection')\n",
        "                    state_selection.click()\n",
        "                    #time.sleep(7)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "                    # clicking on cities box\n",
        "                    cities_box = WebDriverWait(driver, 20).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"cities_taglist\"]'))\n",
        "                    )\n",
        "                    print(\"Clicking on the cities filter box\")\n",
        "                    cities_box.click()\n",
        "                    # time.sleep(7)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "                    # states_box.click()\n",
        "                    # driver.find_element(by=By.XPATH, value='//*[@id=\"powerCities\"]/div[4]/div/input').click()\n",
        "\n",
        "                    # selecting city\n",
        "                    print(\"Selecting the city\")\n",
        "                    WebDriverWait(driver, 30).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, City_x_path))\n",
        "                    ).click()\n",
        "                    # driver.find_element(by=By.XPATH, value=City_x_path).click()\n",
        "                    # time.sleep(10)\n",
        "                    print(\"Loading...\")\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "\n",
        "                    # clicking on source box\n",
        "                    print(\"Clicking on the source box\")\n",
        "\n",
        "                    WebDriverWait(driver, 20).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"sources_taglist\"]'))\n",
        "                    ).click()\n",
        "\n",
        "                    # driver.find_element(by=By.XPATH, value='//*[@id=\"srcWrapper\"]/div/div').click()\n",
        "                    # time.sleep(10)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "                    # selecting source\n",
        "                    print(\"Selecting the source\")\n",
        "                    WebDriverWait(driver, 20).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, Source_x_path))\n",
        "                    ).click()\n",
        "                    # driver.find_element(by=By.XPATH, value=Source_x_path).click()\n",
        "                    # time.sleep(7)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "                    # clicking on date box\n",
        "                    print(\"Clicking on date box\")\n",
        "                    WebDriverWait(driver, 20).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"datePrograms\"]'))\n",
        "                    ).click()\n",
        "                    # driver.find_element(by=By.XPATH, value='//*[@id=\"datePrograms\"]').click()\n",
        "                    # time.sleep(7)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "                    # clearing default date from date box\n",
        "                    print(\"Clearing date box\")\n",
        "                    date_box = WebDriverWait(driver, 20).until(\n",
        "                        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"datePrograms\"]'))\n",
        "                    )\n",
        "                    date_box.clear()\n",
        "                    # driver.find_element(by=By.XPATH, value='//*[@id=\"datePrograms\"]').clear()\n",
        "                    # time.sleep(5)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "                    # sending date\n",
        "                    print(f\"Entering desired date {Date_str} into date box\")\n",
        "                    date_box.send_keys(Date_str)\n",
        "                    # driver.find_element(by=By.XPATH, value='//*[@id=\"datePrograms\"]').send_keys(Date_str)\n",
        "                    # time.sleep(7)\n",
        "                    WebDriverWait(driver, 10).until(\n",
        "                        EC.invisibility_of_element_located((By.CLASS_NAME, 'js-spin-overlay'))\n",
        "                    )\n",
        "\n",
        "                    # UPDATE 10/22/23\n",
        "                    ob = Screenshot.Screenshot()\n",
        "                    img_url = ob.full_screenshot(driver, save_path=r'.', image_name='myimage.png', is_load_at_runtime=True,load_wait_time=3)\n",
        "\n",
        "\n",
        "                    # clicking submit\n",
        "                    print('Finding submit button')\n",
        "                    # submit_button = driver.find_element(by=By.XPATH, value='//*[@id=\"btnListPrograms\"]')\n",
        "                    # submit_button = driver.find_element(By.ID, \"btnListPrograms\")\n",
        "                    submit_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'btnListPrograms')))\n",
        "\n",
        "                    print('found submit button')\n",
        "                    time.sleep(1)\n",
        "                    # driver.execute_script(\"document.getElementById('btnListPrograms').click();\")\n",
        "                    submit_button.click()\n",
        "                    print(\"submit complete\")\n",
        "                    time.sleep(1)\n",
        "\n",
        "\n",
        "                    # WebDriverWait(driver, 10).until(\n",
        "                    #     EC.presence_of_element_located((By.ID, 'js-spin-spinner'))\n",
        "                    # )\n",
        "                    try:\n",
        "                      WebDriverWait(driver, 120).until(\n",
        "                          EC.invisibility_of_element_located((By.XPATH, '/html/div[2]'))\n",
        "                      )\n",
        "                    except Exception as e:\n",
        "                      print(e)\n",
        "                      print(\"Submit timeout occurred, not attempting query again\")\n",
        "                      successful_query.append([Market, Source, Date_str])\n",
        "                      Submit_Timeout = True\n",
        "                      Query_Success = False\n",
        "\n",
        "\n",
        "                    found = False\n",
        "                    if not Submit_Timeout:\n",
        "                      print(\"looking for transcripts\")\n",
        "                      try:\n",
        "                          # We want to detect the message box when it is\n",
        "                          # <div class=\"success\" style=\"display: block;\">No programs found.</div>\n",
        "                          # but not:\n",
        "                          # <div class=\"success\" style=\"display: none;\">No programs found.</div>\n",
        "                          WebDriverWait(driver, 20).until(\n",
        "                              EC.presence_of_element_located((By.XPATH,'//div[@class=\"success\" and @style=\"display: block;\"]'))\n",
        "                          )\n",
        "\n",
        "                          print(\"no transcripts found\")\n",
        "                          found = False\n",
        "                          num_failed_attempt = 3\n",
        "                          print(f\"Attempt Failed: query #{query_idx} attempt #{num_failed_attempt}\")\n",
        "                          print(\"Writing successful query (regardless of no transcripts found) to drive\")\n",
        "                          df_successful = pd.DataFrame(successful_query, columns=['Market','Source', 'Date'])\n",
        "                          if pathlib.Path(crawled_file).is_file(): # append rows without header if failed_query_file already exists\n",
        "                              df_successful.to_csv(crawled_file, index=False, mode='a', header=False)\n",
        "                          else:\n",
        "                              df_successful.to_csv(crawled_file, index=False)\n",
        "\n",
        "                      except UnexpectedAlertPresentException as alert_exception:\n",
        "                          # Handle the unexpected alert\n",
        "                          print(f\"Unexpected alert: {alert_exception}\")\n",
        "                          # You can choose to accept, dismiss, or perform other actions with the alert here.\n",
        "                          # For example, to accept the alert:\n",
        "                          driver.switch_to.alert.accept()\n",
        "                      except TimeoutException as e:\n",
        "                          found = True\n",
        "\n",
        "                    if found:\n",
        "                        # Wait until the first row of the results table appears\n",
        "                        # Note that the NDS website seems to create the table itself before it fills\n",
        "                        # in the rows, so we can't rely on the tbody's existence to tell us\n",
        "                        # when the results have loaded\n",
        "                        print('Waiting for results to load...')\n",
        "                        results_table = WebDriverWait(driver, 120).until(\n",
        "                            EC.presence_of_element_located((By.XPATH, '//*[@id=\"results\"]/table/tbody/tr[1]')),\n",
        "                        )\n",
        "                        print('Results have loaded or else it timed')\n",
        "\n",
        "                        # collect all urls from the query\n",
        "                        next_row_exists = True\n",
        "\n",
        "                        i = 0 #Note that XPath indexing starts at 1\n",
        "                        while next_row_exists:\n",
        "                            i = i + 1\n",
        "                            try:\n",
        "                                # extract link\n",
        "                                xpath = '//*[@id=\"results\"]/table/tbody/tr[' + str(i) + ']/td[5]/a'\n",
        "                                onclick = driver.find_element(by=By.XPATH, value=xpath).get_attribute('onclick')\n",
        "                                url = onclick.split(\"'\")[1]\n",
        "\n",
        "                                # extract time and title of the program (from the web page listing search results) with the url\n",
        "                                Time_str = driver.find_element(by=By.XPATH, value='//*[@id=\"results\"]/table/tbody/tr[' + str(i) + ']/td[3]').text\n",
        "                                Title_str = driver.find_element(by=By.XPATH, value='//*[@id=\"results\"]/table/tbody/tr[' + str(i) + ']/td[4]').text\n",
        "\n",
        "                                # attach the program identification and URL to the nested list\n",
        "                                programs.append([Date_str, Time_str, Title_str, Source, Market, url, False])\n",
        "                            except:\n",
        "                                # if the next link does not exist, set b to false and exit query\n",
        "                                next_row_exists = False\n",
        "\n",
        "                        print(\"num links collected: \" + str(len(programs)))\n",
        "                        # time.sleep(3)\n",
        "\n",
        "                        Query_Success = True # the query attempt ran without error\n",
        "                        if len(programs) > 0:\n",
        "                            links = links + programs # add the programs from this success query\n",
        "                            #print(\"APPENDING\" +str(Market)+\", \"+str(Source)+\", \"+str(Date_str) +\" to succeded queries...\")\n",
        "                            successful_query.append([Market, Source, Date_str])\n",
        "                        else:\n",
        "                            #print(\"APPENDING\" +str(Market)+\", \"+str(Source)+\", \"+str(Date_str) +\" to failed queries...\")\n",
        "                            failed_query.append([Date_str, Source, Market, 'no link']) # no program is found\n",
        "\n",
        "                except Exception as e: # the query attempt has failed\n",
        "                    template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
        "                    message = template.format(type(e).__name__, e.args)\n",
        "                    print(Back.RED + Fore.WHITE + message)\n",
        "                    print(Style.RESET_ALL)\n",
        "                    ob = Screenshot.Screenshot()\n",
        "                    img_url = ob.full_screenshot(driver, save_path=r'.', image_name='error_debug.png', is_load_at_runtime=True,load_wait_time=3)\n",
        "                    # print(e)\n",
        "\n",
        "                    num_failed_attempt += 1\n",
        "                    print(f\"Attempt Failed: query #{query_idx} attempt #{num_failed_attempt}\")\n",
        "\n",
        "            if not Query_Success: # all attempts failed\n",
        "                Driver_Success = False # encountered a fatal failure\n",
        "                if query_idx == num_query:\n",
        "                  failed_query.append([Date_str, Source, Market, '3 failed attempts'])\n",
        "                else:\n",
        "                  if Submit_Timeout:\n",
        "                    failed_query.append([Date_str, Source, Market, 'submission timeout occurred'])\n",
        "                    # print(\"Writing successful query due to submit timeout\")\n",
        "                    # df_successful = pd.DataFrame(successful_query, columns=['Market','Source', 'Date'])\n",
        "                    # if pathlib.Path(crawled_file).is_file(): # append rows without header if failed_query_file already exists\n",
        "                    #     df_successful.to_csv(crawled_file, index=False, mode='a', header=False)\n",
        "                    # else:\n",
        "                    #     df_successful.to_csv(crawled_file, index=False)\n",
        "                  else:\n",
        "                    failed_query.append([Date_str, Source, Market, 'no transcripts found'])\n",
        "                print(Back.RED + Fore.WHITE + f\"Query {query_idx} has failed. Quitting and restarting webdriver\")\n",
        "                print(Style.RESET_ALL)\n",
        "                driver.quit()\n",
        "\n",
        "            print(\"Writing URLS to drive...\")\n",
        "            df_urls = pd.DataFrame(links, columns=['Date', 'Time', 'Title', 'Source', 'Market', 'URL', 'Scraped']).drop_duplicates().reset_index(drop=True)\n",
        "            if pathlib.Path(urls_file).is_file(): # append rows without header if urls_file already exists\n",
        "                df_urls.to_csv(urls_file, index=False, mode='a', header=False)\n",
        "            else:\n",
        "                df_urls.to_csv(urls_file, index=False)\n",
        "\n",
        "            print(\"Writing top 100 URLs to drive...\")\n",
        "            if pathlib.Path(urls_file_100).is_file(): # append rows without   if urls_file already exists for each 100 rows\n",
        "                df_urls.to_csv(urls_file_100, index=False)\n",
        "            else:\n",
        "                df_urls.to_csv(urls_file_100,index = False)\n",
        "\n",
        "            # Writing successful queries\n",
        "            print(\"Writing successful query (if any) to drive\")\n",
        "            df_successful = pd.DataFrame(successful_query, columns=['Market','Source', 'Date'])\n",
        "            if pathlib.Path(crawled_file).is_file(): # append rows without header if failed_query_file already exists\n",
        "                df_successful.to_csv(crawled_file, index=False, mode='a', header=False)\n",
        "            else:\n",
        "                df_successful.to_csv(crawled_file, index=False)\n",
        "\n",
        "            # writing queries that encountered a fatal failure\n",
        "            print(\"Writing failed query (if any) to drive\")\n",
        "            df_failed = pd.DataFrame(failed_query, columns=['Date', 'Source', 'Market', 'Error'])\n",
        "            if pathlib.Path(failed_query_file).is_file(): # append rows without header if failed_query_file already exists\n",
        "                df_failed.to_csv(failed_query_file, index=False, mode='a', header=False)\n",
        "            else:\n",
        "                df_failed.to_csv(failed_query_file, index=False)\n",
        "            links.clear()\n",
        "            successful_query.clear()\n",
        "            failed_query.clear()\n",
        "\n",
        "            query_idx += 1 # move on to the next query\n",
        "\n",
        "    driver.quit()\n",
        "    print(\"All queries complete\")\n",
        "\n",
        "    # write obtained URLs to a csv file\n",
        "    # Note: Date & Source & Market is from programs_file, Time & Title & URL is from the web page listing search results\n",
        "    # df_urls = pd.DataFrame(links, columns=['Date', 'Time', 'Title', 'Source', 'Market', 'URL', 'Scraped']).drop_duplicates().reset_index(drop=True)\n",
        "    # if pathlib.Path(urls_file).is_file(): # append rows without header if urls_file already exists\n",
        "    #     df_urls.to_csv(urls_file, index=False, mode='a', header=False)\n",
        "    # else:\n",
        "    #     df_urls.to_csv(urls_file, index=False)\n",
        "\n",
        "    # if pathlib.Path(urls_file_100).is_file(): # append rows without header if urls_file already exists for each 100 rows\n",
        "    #     df_urls.to_csv(urls_file_100, index=False)\n",
        "    # else:\n",
        "    #     df_urls.to_csv(urls_file_100,index = False)\n",
        "\n",
        "    # # writing queries that encountered a fatal failure\n",
        "    # df_failed = pd.DataFrame(failed_query, columns=['Date', 'Source', 'Market', 'Error'])\n",
        "    # if pathlib.Path(failed_query_file).is_file(): # append rows without header if failed_query_file already exists\n",
        "    #     df_failed.to_csv(failed_query_file, index=False, mode='a', header=False)\n",
        "    # else:\n",
        "    #     df_failed.to_csv(failed_query_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaHTETvud2ua"
      },
      "source": [
        "## select_urls.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlG9XxFEeAIb"
      },
      "outputs": [],
      "source": [
        "# Given all the links from a query (urls_file) and the list of required programs (programs_file),\n",
        "# returns a csv file of just the required urls\n",
        "# Last execution: Sep 2021, Google Colab\n",
        "\n",
        "# parse url to get time and title of the program\n",
        "def get_Time_and_Title(nds_url_str):\n",
        "    prog_datetime = nds_url_str.split('&')[2].split('=')[-1]\n",
        "    prog_title = nds_url_str.split('&')[4].split('=')[-1].replace('%20', ' ').replace('%40', '@')\n",
        "    return prog_datetime[11:], prog_title\n",
        "\n",
        "\n",
        "def select_urls(urls_file, programs_file):\n",
        "    # df_prog has column names Date, Time, Title, Source, Market, URL, Scraped in this order\n",
        "    df_prog = pd.read_csv(programs_file)\n",
        "    df_url = pd.read_csv(urls_file)\n",
        "    # sort two dataframes in the same order\n",
        "    for df in [df_prog, df_url]:\n",
        "        df.sort_values(['Market', 'Source', 'Date', 'Time'], inplace=True, ignore_index=True)\n",
        "\n",
        "    # insert missing time and title (extracted from the url) if one of them is missing\n",
        "    df_missing = df_url.loc[df_url['Time'].isnull() | df_url['Title'].isnull()]\n",
        "    for ind, row in df_missing.iterrows():\n",
        "        prog_time, prog_title = get_Time_and_Title(row['URL'])\n",
        "        df_url.loc[ind, 'Time'] = prog_time\n",
        "        df_url.loc[ind, 'Title'] = prog_title\n",
        "\n",
        "    df_prog['URL'] = ''\n",
        "\n",
        "    search_start = 0\n",
        "    for i in range(df_prog.shape[0]):\n",
        "        ith_row_id = df_prog.loc[i, ['Date', 'Time', 'Title', 'Source', 'Market']].values\n",
        "        for j in range(df_url.shape[0]):\n",
        "            jth_row_id = df_url.loc[j, ['Date', 'Time', 'Title', 'Source', 'Market']].values\n",
        "            if all(ith_row_id == jth_row_id):\n",
        "                df_prog.loc[i, 'URL'] = df_url.loc[j, 'URL'] #copy url\n",
        "                break\n",
        "\n",
        "    df_prog.to_csv(programs_file, index=False)\n",
        "    df_url.to_csv(urls_file, index=False)\n",
        "\n",
        "\n",
        "# use this function instead of select_urls() if need non-program-specific transcripts\n",
        "def fill_in_missing_time_title(urls_file):\n",
        "  df_url = pd.read_csv(urls_file)\n",
        "  df_missing = df_url.loc[df_url['Time'].isnull() | df_url['Title'].isnull()]\n",
        "  for ind, row in df_missing.iterrows():\n",
        "      prog_time, prog_title = get_Time_and_Title(row['URL'])\n",
        "      df_url.loc[ind, 'Time'] = prog_time\n",
        "      df_url.loc[ind, 'Title'] = prog_title\n",
        "  df_url.to_csv(urls_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEr2i4cveG1A"
      },
      "source": [
        "## nds_scraper.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvfZ38lDeMDf"
      },
      "outputs": [],
      "source": [
        "# This file is used to iterate through each url,\n",
        "# scrape the text within the url, and store it in the directory we want.\n",
        "# Last execution: Sep 2021, Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6RusHAYgRlM"
      },
      "outputs": [],
      "source": [
        "# format the date for filename\n",
        "def return_Date_str(Date):\n",
        "\n",
        "    monthToNum = {\n",
        "        'JAN' : '1',\n",
        "        'FEB' : '2',\n",
        "        'MAR' : '3',\n",
        "        'APR' : '4',\n",
        "        'MAY' : '5',\n",
        "        'JUN' : '6',\n",
        "        'JUL' : '7',\n",
        "        'AUG' : '8',\n",
        "        'SEP' : '9',\n",
        "        'OCT' : '10',\n",
        "        'NOV' : '11',\n",
        "        'DEC' : '12'\n",
        "    }\n",
        "\n",
        "    parse_date = Date.split(\" \")\n",
        "\n",
        "    month = monthToNum[parse_date[0]]\n",
        "    date = parse_date[1]\n",
        "    year = parse_date[2]\n",
        "\n",
        "    return year + \"-\" + month + \"-\" + date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stCEUZ4ugWyD"
      },
      "outputs": [],
      "source": [
        "def nds_scrape(programs_file, unscraped_programs_file, transcripts_dir, driver_option):\n",
        "    df = pd.read_csv(programs_file)\n",
        "    df[\"Error\"] = ''\n",
        "    driver = webdriver.Chrome(options=driver_option)\n",
        "\n",
        "    print(\"scrape total of {} indices\".format(df.shape[0]))\n",
        "    df = df.drop_duplicates(subset = \"URL\")\n",
        "\n",
        "    #df = df[df.Source == 'ABC News']\n",
        "    # iterate through all of the links\n",
        "    for index, row in df.iterrows():\n",
        "        if row['URL'] == '':\n",
        "            print(\"index {} has missing URL\".format(index))\n",
        "        elif row['Scraped']:\n",
        "            print(\"index {} has already been scraped\".format(index))\n",
        "        else:\n",
        "            try:\n",
        "                print(\"scraping text from index {}\".format(index))\n",
        "                # go to link\n",
        "                driver.get(row['URL'])\n",
        "                # # extract text from the body of the website\n",
        "                body = driver.find_element(By.XPATH, \"/html/body/table/tbody\").text\n",
        "                # # split body of the text by line\n",
        "                # split = body.splitlines()\n",
        "                # # extract station\n",
        "                # station = split[0].strip()\n",
        "                # # extract date and broadcast\n",
        "                # date_broadcast = split[1]\n",
        "                # date_broadcast_split = date_broadcast.split(\"  \")\n",
        "                # date = return_Date_str(date_broadcast_split[0]) #2008-7-28\n",
        "                # broadcast = date_broadcast_split[1].strip()\n",
        "\n",
        "                # # extract the first timestamp\n",
        "                # first_paragraph = split[2].split(\" \")\n",
        "                # time = first_paragraph[0][3:]\n",
        "                # time = time.replace(\":\", \"_\").strip()\n",
        "                # am_pm = first_paragraph[1][:2].strip()\n",
        "\n",
        "                # # save the text of the body to a .txt file in the specified directory\n",
        "                # # filename = e.g. 'KARE 2008-7-28 04_00_02PM KARE 11 AT 4.txt'\n",
        "                # filename = station + \" \" + date + \" \" + time + am_pm + \" \" + broadcast + \".txt\"\n",
        "                station, month, date, fileName = downloaded_file_name_formatter(row)\n",
        "                fullDir = '{}/{}/{}/{}'.format(transcripts_dir, station, month, date)\n",
        "                ensure_dir(fullDir) # ensure that the directories for the mp4 exist\n",
        "                fullPath = '{}/{}/{}/{}/{}.txt'.format(transcripts_dir, station, month, date, fileName)\n",
        "                # if fullPath already exists, raise an error\n",
        "                assert (not pathlib.Path(fullPath).is_file()), \"File '{}' already exists\\n\".format(fileName)\n",
        "\n",
        "\n",
        "                # # specify directory\n",
        "                # file_dir = '{}/{}'.format(transcripts_dir, station)\n",
        "                # if not os.path.exists(file_dir):\n",
        "                #     os.makedirs(file_dir)\n",
        "                # filepath = os.path.join(file_dir, filename)\n",
        "                file = open(fullPath, \"w\")\n",
        "                file.write(body)\n",
        "                file.close()\n",
        "                df.loc[index, 'Scraped'] = True\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(\"failed to scrape text from index {}\".format(index))\n",
        "                df.loc[index, 'Error'] = e\n",
        "\n",
        "        if (index + 1)%50==0: # update csv file after 50 iterations\n",
        "            df_unscraped = df[~df['Scraped']] # programs that were not scraped\n",
        "            df.to_csv(programs_file, index=False)\n",
        "            df_unscraped.to_csv(unscraped_programs_file, index=False)\n",
        "\n",
        "    df_unscraped = df[~df['Scraped']] # programs that were not scraped\n",
        "    df.to_csv(programs_file, index=False)\n",
        "    df_unscraped.to_csv(unscraped_programs_file, index=False)\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FijMgR1fh80K"
      },
      "outputs": [],
      "source": [
        "# checks if the path specified by file_dir is a existing directory, if not then create it (recursively)\n",
        "def ensure_dir(file_dir):\n",
        "    if not pathlib.Path(file_dir).is_dir():\n",
        "        pathlib.Path(file_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18b9y_5ahV-U"
      },
      "outputs": [],
      "source": [
        "# format parts of the file path to match the existing folders & files:\n",
        "# currently, path would be station/month/date/fileName\n",
        "def downloaded_file_name_formatter(row):\n",
        "    station = row['Source']\n",
        "\n",
        "    monthNum = row['Date'].split('-')[1]\n",
        "    month_conversion = {'01':'January', '02':'February', '03':'March', '04':'April', '05':'May', '06':'June', '07':'July', '08':'August', '09':'September', '10':'October', '11':'November', '12':'December'}\n",
        "    assert (monthNum in month_conversion),  ('Error reading program month:' + month + 'read\\n')\n",
        "    month = month_conversion.get(monthNum)\n",
        "\n",
        "    # date example: 042321\n",
        "    year = row['Date'].split('-')[0][2:]\n",
        "    day = row['Date'].split('-')[2]\n",
        "    date = monthNum + day + year\n",
        "    # fileName example: FOX News 041421 400PM CT_1 (the \"_1\" means part 1)\n",
        "\n",
        "    # first read the part number of the program title (may be empty)\n",
        "    partNum = \"\"\n",
        "    # ptSubstring_index is the index of the substring 'pt' in the program name\n",
        "    ptSubstring_index = row['Title'].find('pt', -5, len(row['Title']))\n",
        "    if ptSubstring_index != -1:\n",
        "      # both 'pt.3' and 'pt 3' can appear in program names\n",
        "      partNum = row['Title'][ptSubstring_index + 3]\n",
        "      # deal with 'pt. 3'\n",
        "      if partNum == \" \":\n",
        "        partNum = row['Title'][ptSubstring_index + 4]\n",
        "      assert partNum.isnumeric(), ('Error reading program part number:' + partNum + 'read\\n')\n",
        "      partNum = '_' + partNum\n",
        "\n",
        "    time_raw = row['Time'].split()\n",
        "    timeZone = \"\"\n",
        "    try:\n",
        "      time = time_raw[0]\n",
        "      timeZone = time_raw[1]\n",
        "    except Exception as e:\n",
        "      print(\"An error occured while fetching the time zone: \",e)\n",
        "    fileName = \" \".join([station, date, time+' '+timeZone+partNum])\n",
        "\n",
        "    return station, month, date, fileName"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbqM20IU46oS"
      },
      "source": [
        "# Run the main function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT3khpdJeURp"
      },
      "source": [
        "## nds_main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de_wsGWPoCnB"
      },
      "source": [
        "**Create programs_file using dataframe:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsqL3k8K7W81"
      },
      "outputs": [],
      "source": [
        "# local_sources = [\n",
        "#     'CBSN Minneapolis', 'KARE', 'KCCO-AM', 'KDWB-FM', 'KEEY-FM',\n",
        "#     'KFAN-FM', 'KFXN-FM','KLTF-AM','KMSP','KNOW-FM',\n",
        "#     'KNSI-AM', 'KOWZ-FM','KQQL-FM', 'KQRS-FM', 'KSTC',\n",
        "#     'KSTP','KST-AM','KTCA', 'KTCI','KTCZ-FM',\n",
        "#     'KTLK-AM', 'KTMY-FM','KTWN-FM','TPT','WCCO',\n",
        "#     'WCCO-AM','WFTC','WMNN-AM','WUCW', 'WUMN',\n",
        "#     'WVAL-AM'\n",
        "# ]\n",
        "\n",
        "local_sources = ['WCCO', 'KARE', 'KMSP', 'KSTP', 'TPT']\n",
        "\n",
        "\n",
        "national_news_network_sources = [\n",
        "    'ABC', 'CBS', 'CNN', 'FOX', 'FOXNEWS', 'MSNBC', 'NBC'\n",
        "]\n",
        "national_news_all_other_sources = [\n",
        "    'ABC', 'ABC News', 'CBS', 'CBS News', 'CNN', 'FOX', 'FOXNEWS', 'MSNBC',\n",
        "    'NBC', 'NBC News Now', 'PBS'\n",
        "]\n",
        "\n",
        "# local_sources = ['KSTP']\n",
        "\n",
        "# national_news_network_sources = []\n",
        "\n",
        "# national_news_all_other_sources = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1hWK6zxKjmzy",
        "outputId": "5ef314e8-5b90-4358-f6d1-d56b82cef0e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Market Source       Date\n",
              "0       National News Networks    ABC 2019-11-01\n",
              "1       National News Networks    ABC 2019-11-02\n",
              "2       National News Networks    ABC 2019-11-03\n",
              "3       National News Networks    ABC 2019-11-04\n",
              "4       National News Networks    ABC 2019-11-05\n",
              "...                        ...    ...        ...\n",
              "1398  Minneapolis-St. Paul, MN    TPT 2019-12-27\n",
              "1399  Minneapolis-St. Paul, MN    TPT 2019-12-28\n",
              "1400  Minneapolis-St. Paul, MN    TPT 2019-12-29\n",
              "1401  Minneapolis-St. Paul, MN    TPT 2019-12-30\n",
              "1402  Minneapolis-St. Paul, MN    TPT 2019-12-31\n",
              "\n",
              "[1403 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59cd3193-3019-4aaa-a824-29500b9529a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Market</th>\n",
              "      <th>Source</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>ABC</td>\n",
              "      <td>2019-11-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>ABC</td>\n",
              "      <td>2019-11-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>ABC</td>\n",
              "      <td>2019-11-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>ABC</td>\n",
              "      <td>2019-11-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>ABC</td>\n",
              "      <td>2019-11-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>TPT</td>\n",
              "      <td>2019-12-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>TPT</td>\n",
              "      <td>2019-12-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>TPT</td>\n",
              "      <td>2019-12-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1401</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>TPT</td>\n",
              "      <td>2019-12-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1402</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>TPT</td>\n",
              "      <td>2019-12-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1403 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59cd3193-3019-4aaa-a824-29500b9529a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59cd3193-3019-4aaa-a824-29500b9529a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59cd3193-3019-4aaa-a824-29500b9529a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e6426e16-7a14-4cc1-a9a6-835ea73b4e54\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6426e16-7a14-4cc1-a9a6-835ea73b4e54')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e6426e16-7a14-4cc1-a9a6-835ea73b4e54 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\n",
        "# UPDATED BY AARON BRONSTONE 10/13/2023\n",
        "date_range1 = list(pd.date_range(\n",
        "    start='08/26/2004',\n",
        "    end='08/27/2004'\n",
        "))\n",
        "\n",
        "date_range2 = list(pd.date_range(\n",
        "    start='11/01/2019',\n",
        "    end='12/31/2019'\n",
        "))\n",
        "\n",
        "date_range3 = list(pd.date_range(\n",
        "    start='10/13/2004',\n",
        "    end='10/17/2004'\n",
        "))\n",
        "\n",
        "date_range4 = list(pd.date_range(\n",
        "    start='10/30/2004',\n",
        "    end='11/1/2004'\n",
        "))\n",
        "\n",
        "#date_range = date_range1 + date_range2 + date_range3 + date_range4\n",
        "date_range = date_range2\n",
        "\n",
        "\n",
        "# Create a dataframe of the cartesian product of sources and dates\n",
        "# aka each unique combination of the two\n",
        "\n",
        "df_local_networks = pd.DataFrame(\n",
        "    product(['Minneapolis-St. Paul, MN'], local_sources, date_range),\n",
        "    columns=['Market', 'Source', 'Date']\n",
        ")\n",
        "\n",
        "df_national_national_networks = pd.DataFrame(\n",
        "    product(['National News Networks'], national_news_network_sources, date_range),\n",
        "    columns=['Market', 'Source', 'Date']\n",
        ")\n",
        "\n",
        "df_national_all_other_networks = pd.DataFrame(\n",
        "    product(['National Networks (All Other)'], national_news_all_other_sources, date_range),\n",
        "    columns = ['Market', 'Source', 'Date']\n",
        ")\n",
        "\n",
        "\n",
        "df_national_combined = pd.concat([df_national_national_networks, df_national_all_other_networks])\n",
        "df_all = pd.concat([df_national_combined, df_local_networks])\n",
        "df_all = df_all.reset_index(drop=True)\n",
        "\n",
        "df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVLx73M7eYuX"
      },
      "outputs": [],
      "source": [
        "# options to make webdriver run in the background\n",
        "op = Options()\n",
        "op.add_argument(\"--disable-gpu\")\n",
        "op.add_argument(\"--disable-extensions\")\n",
        "op.add_argument(\"--proxy-server='direct://'\")\n",
        "op.add_argument(\"--proxy-bypass-list=*\")\n",
        "# op.add_argument(\"--start-maximized\")\n",
        "op.add_argument(\"--headless\")\n",
        "op.add_argument('-no-sandbox')\n",
        "op.add_argument('-disable-dev-shm-usage')\n",
        "op.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "\n",
        "username = 'carletonba'\n",
        "password = 'carletonba'\n",
        "\n",
        "#MODIFY FILEPATHs\n",
        "\n",
        "#CHANGE THIS\n",
        "nds_xls = './programs_1100_1209.csv' # Excel file downloaded from NDS\n",
        "programs_file = './programs_1100_1209.csv' # csv file to store programs\n",
        "\n",
        "#CHANGE THIS\n",
        "urls_file_100 = './scraped_transcripts_1100_1209.csv'\n",
        "\n",
        "#Don't Change this\n",
        "crawled_file = './crawled_queries.csv' # csv file to store all successful queries that have already been crawled\n",
        "urls_file = './scraped_transcripts.csv' # csv file to store all the urls from queries related to each program\n",
        "failed_query_file = './failed_query.csv' # csv file to write which queries have encountered fatal error\n",
        "unscraped_programs_file = './unscraped.csv' # csv file to store programs that need to be scraped.\n",
        "transcripts_dir = '.'\n",
        "\n",
        "if 'REPLACE_THIS' in [username, password]:\n",
        "    sys.exit('Please provide valid NDS login information.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "Txk8Xpbh416H",
        "outputId": "66767bb5-7e12-4d6d-974b-237b8f54316b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRAWLED FILE FOUND\n",
            "Market             object\n",
            "Source             object\n",
            "Date       datetime64[ns]\n",
            "Scraped              bool\n",
            "dtype: object\n",
            "Market            object\n",
            "Source            object\n",
            "Date      datetime64[ns]\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-5b6eb941338f>:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  non_matching_rows_df1 = df_sorted[merged['_merge'] == 'left_only'].copy()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Market Source       Date  Scraped\n",
              "38    Minneapolis-St. Paul, MN   KARE 2019-12-09    False\n",
              "62    Minneapolis-St. Paul, MN   KMSP 2019-11-02    False\n",
              "99    Minneapolis-St. Paul, MN   KMSP 2019-12-09    False\n",
              "131   Minneapolis-St. Paul, MN   KSTP 2019-11-10    False\n",
              "134   Minneapolis-St. Paul, MN   KSTP 2019-11-13    False\n",
              "...                        ...    ...        ...      ...\n",
              "1398    National News Networks    NBC 2019-12-27    False\n",
              "1399    National News Networks    NBC 2019-12-28    False\n",
              "1400    National News Networks    NBC 2019-12-29    False\n",
              "1401    National News Networks    NBC 2019-12-30    False\n",
              "1402    National News Networks    NBC 2019-12-31    False\n",
              "\n",
              "[698 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-676d1b58-b39a-4f82-b9c0-1414c26476ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Market</th>\n",
              "      <th>Source</th>\n",
              "      <th>Date</th>\n",
              "      <th>Scraped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KARE</td>\n",
              "      <td>2019-12-09</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KMSP</td>\n",
              "      <td>2019-11-02</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KMSP</td>\n",
              "      <td>2019-12-09</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KSTP</td>\n",
              "      <td>2019-11-10</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KSTP</td>\n",
              "      <td>2019-11-13</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-27</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-28</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-29</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1401</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-30</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1402</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>698 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-676d1b58-b39a-4f82-b9c0-1414c26476ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-676d1b58-b39a-4f82-b9c0-1414c26476ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-676d1b58-b39a-4f82-b9c0-1414c26476ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0e172a1e-7343-411a-beef-80dd8ba1befa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e172a1e-7343-411a-beef-80dd8ba1befa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0e172a1e-7343-411a-beef-80dd8ba1befa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df_sorted = df_all\n",
        "\n",
        "# # An example data frame where NDS has no transcripts\n",
        "# df_sorted = pd.DataFrame({\n",
        "#     'Market': ['National Networks (All Other)'] ,\n",
        "#     'Source': ['CNN'],\n",
        "#     'Date': ['05/29/2020']\n",
        "# })\n",
        "\n",
        "\n",
        "df_sorted['Scraped'] = False\n",
        "df_sorted = df_sorted.sort_values(['Market', 'Source', 'Date']).reset_index(drop=True)\n",
        "df_sorted.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Remove duplicates from failed query file\n",
        "df_failed = pd.read_csv(failed_query_file)\n",
        "df_failed.drop_duplicates()\n",
        "df_failed = df_failed.sort_values(['Market', 'Source', 'Date']).reset_index(drop=True)\n",
        "df_failed.to_csv(failed_query_file, index=False)\n",
        "\n",
        "if pathlib.Path(crawled_file).is_file():\n",
        "  print(\"CRAWLED FILE FOUND\")\n",
        "  df_already_crawled = pd.read_csv(crawled_file)\n",
        "  df_already_crawled.drop_duplicates()\n",
        "  df_already_crawled['Date'] = pd.to_datetime(df_already_crawled['Date'])\n",
        "  print(df_sorted.dtypes)\n",
        "  print(df_already_crawled.dtypes)\n",
        "  df_already_crawled = df_already_crawled.sort_values(['Market', 'Source', 'Date']).reset_index(drop=True)\n",
        "  df_already_crawled.to_csv(crawled_file, index=False)\n",
        "\n",
        "  columns_to_compare = ['Market','Source','Date']\n",
        "\n",
        "  merged = df_sorted.merge(df_already_crawled, on=columns_to_compare, how='left', indicator=True)\n",
        "  # print(df_sorted[0:30])\n",
        "  # print(merged[0:30])\n",
        "  # Filter the rows in df1 that do not have a match in df2\n",
        "  non_matching_rows_df1 = df_sorted[merged['_merge'] == 'left_only'].copy()\n",
        "  # print(non_matching_rows_df1)\n",
        "  df_sorted = non_matching_rows_df1\n",
        "  # Drop the indicator column '_merge' from the result\n",
        "  #df_sorted = non_matching_rows_df1.drop(columns=['_merge'])\n",
        "\n",
        "#500 - 1000\n",
        "df_sorted_100 = df_sorted.iloc[0:]\n",
        "\n",
        "df_sorted_100.to_csv('./programs_1100_1209.csv', index=False)\n",
        "df_sorted_100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3bvjBnW2aLZ5",
        "outputId": "22bec105-8e8b-4b72-fa90-b13e7e45594c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Market Source       Date  Scraped\n",
              "38    Minneapolis-St. Paul, MN   KARE 2019-12-09    False\n",
              "62    Minneapolis-St. Paul, MN   KMSP 2019-11-02    False\n",
              "99    Minneapolis-St. Paul, MN   KMSP 2019-12-09    False\n",
              "131   Minneapolis-St. Paul, MN   KSTP 2019-11-10    False\n",
              "134   Minneapolis-St. Paul, MN   KSTP 2019-11-13    False\n",
              "...                        ...    ...        ...      ...\n",
              "1398    National News Networks    NBC 2019-12-27    False\n",
              "1399    National News Networks    NBC 2019-12-28    False\n",
              "1400    National News Networks    NBC 2019-12-29    False\n",
              "1401    National News Networks    NBC 2019-12-30    False\n",
              "1402    National News Networks    NBC 2019-12-31    False\n",
              "\n",
              "[698 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-232e7b42-cac4-4e29-9fec-a70ee0d46da2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Market</th>\n",
              "      <th>Source</th>\n",
              "      <th>Date</th>\n",
              "      <th>Scraped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KARE</td>\n",
              "      <td>2019-12-09</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KMSP</td>\n",
              "      <td>2019-11-02</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KMSP</td>\n",
              "      <td>2019-12-09</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KSTP</td>\n",
              "      <td>2019-11-10</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>Minneapolis-St. Paul, MN</td>\n",
              "      <td>KSTP</td>\n",
              "      <td>2019-11-13</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-27</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-28</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-29</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1401</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-30</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1402</th>\n",
              "      <td>National News Networks</td>\n",
              "      <td>NBC</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>698 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-232e7b42-cac4-4e29-9fec-a70ee0d46da2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-232e7b42-cac4-4e29-9fec-a70ee0d46da2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-232e7b42-cac4-4e29-9fec-a70ee0d46da2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-44d24012-4ad1-4dfd-858e-6d91905dfd34\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44d24012-4ad1-4dfd-858e-6d91905dfd34')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-44d24012-4ad1-4dfd-858e-6d91905dfd34 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df_sorted_100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1J84C6FPbkU"
      },
      "source": [
        "**9/4 by Helen: added two options to the driver: no-sandbox and disable-dev-shm-usage. Honestly I have no idea what they mean and what OS Colab's server uses (some of these options are OS specific), but it seems to work... I also added transcripts_dir as an argument for nds_scrape, just to make it easier to identify & change where we're storing the transcripts.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eLzqLo9W2jQ"
      },
      "source": [
        "### Instructions:\n",
        "\n",
        "1. **if you have the excel file from NDS, and you want to download transcripts for just the selected programs**,  run this cell："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzHHg2dzX8c4"
      },
      "outputs": [],
      "source": [
        "# This section keeps throwing errors. Is this code cell still relevant? 10/11/22\n",
        "# get_unique_program(nds_xls, programs_file)\n",
        "# nds_crawl(username, password, programs_file, urls_file, failed_query_file, driver_option=op)\n",
        "# select_urls(urls_file, programs_file)\n",
        "# nds_scrape(programs_file, unscraped_programs_file, transcripts_dir, driver_option=op)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArIi5qw6YCJa"
      },
      "source": [
        "2. **if you want to download all transcripts for specified Date & Source & Market, after making programs_file (by modifying the first code cell in nds_main.py, or manually creating a csv)**, run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "vFOW-TW6uUXr",
        "outputId": "0b6620f5-e783-4047-b6b3-21167556651f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date Source                    Market\n",
            "0    2019-12-09   KARE  Minneapolis-St. Paul, MN\n",
            "1    2019-11-02   KMSP  Minneapolis-St. Paul, MN\n",
            "2    2019-12-09   KMSP  Minneapolis-St. Paul, MN\n",
            "3    2019-11-10   KSTP  Minneapolis-St. Paul, MN\n",
            "4    2019-11-13   KSTP  Minneapolis-St. Paul, MN\n",
            "..          ...    ...                       ...\n",
            "693  2019-12-27    NBC    National News Networks\n",
            "694  2019-12-28    NBC    National News Networks\n",
            "695  2019-12-29    NBC    National News Networks\n",
            "696  2019-12-30    NBC    National News Networks\n",
            "697  2019-12-31    NBC    National News Networks\n",
            "\n",
            "[698 rows x 3 columns]\n",
            "Making 698 total queries\n",
            "Logging in to new NDS instance...\n",
            "\u001b[30m\n",
            "\u001b[42m\n",
            "=== Query 0, which is for market Minneapolis-St. Paul, MN, source KARE, and date 2019-12-09 ===\n",
            "\u001b[0m\n",
            "Finding the Broadcast Content tab\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8e6648807638>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnds_crawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprograms_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls_file_100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrawled_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_query_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_option\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-06d414c7e6aa>\u001b[0m in \u001b[0;36mnds_crawl\u001b[0;34m(username, password, programs_file, urls_file, urls_file_100, crawled_file, failed_query_file, driver_option)\u001b[0m\n\u001b[1;32m     84\u001b[0m                       \u001b[0;31m# navigating to search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finding the Broadcast Content tab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                       nav_button = WebDriverWait(driver, 20).until(\n\u001b[0m\u001b[1;32m     87\u001b[0m                           \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'//*[@id=\"navigation\"]/li[6]/a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/support/wait.py\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"screen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mstacktrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stacktrace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "nds_crawl(username, password, programs_file, urls_file, urls_file_100, crawled_file, failed_query_file, driver_option=op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vwxKqH7IYGZM",
        "outputId": "27252b28-7625-45bf-82da-490f3d783e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scrape total of 651 indices\n",
            "index 0 has already been scraped\n",
            "index 1 has already been scraped\n",
            "index 2 has already been scraped\n",
            "index 3 has already been scraped\n",
            "index 4 has already been scraped\n",
            "index 5 has already been scraped\n",
            "index 6 has already been scraped\n",
            "index 7 has already been scraped\n",
            "index 8 has already been scraped\n",
            "index 9 has already been scraped\n",
            "index 10 has already been scraped\n",
            "index 11 has already been scraped\n",
            "index 12 has already been scraped\n",
            "index 13 has already been scraped\n",
            "index 14 has already been scraped\n",
            "index 15 has already been scraped\n",
            "index 16 has already been scraped\n",
            "index 17 has already been scraped\n",
            "index 18 has already been scraped\n",
            "index 19 has already been scraped\n",
            "index 20 has already been scraped\n",
            "index 21 has already been scraped\n",
            "index 22 has already been scraped\n",
            "index 23 has already been scraped\n",
            "index 24 has already been scraped\n",
            "index 25 has already been scraped\n",
            "index 26 has already been scraped\n",
            "index 27 has already been scraped\n",
            "index 28 has already been scraped\n",
            "index 29 has already been scraped\n",
            "index 30 has already been scraped\n",
            "index 31 has already been scraped\n",
            "index 32 has already been scraped\n",
            "index 33 has already been scraped\n",
            "index 34 has already been scraped\n",
            "index 35 has already been scraped\n",
            "index 36 has already been scraped\n",
            "index 37 has already been scraped\n",
            "index 38 has already been scraped\n",
            "index 39 has already been scraped\n",
            "index 40 has already been scraped\n",
            "index 41 has already been scraped\n",
            "index 42 has already been scraped\n",
            "index 43 has already been scraped\n",
            "index 44 has already been scraped\n",
            "index 45 has already been scraped\n",
            "index 46 has already been scraped\n",
            "index 47 has already been scraped\n",
            "index 48 has already been scraped\n",
            "index 49 has already been scraped\n",
            "index 50 has already been scraped\n",
            "index 51 has already been scraped\n",
            "index 52 has already been scraped\n",
            "index 53 has already been scraped\n",
            "index 54 has already been scraped\n",
            "index 55 has already been scraped\n",
            "index 56 has already been scraped\n",
            "index 57 has already been scraped\n",
            "index 58 has already been scraped\n",
            "index 59 has already been scraped\n",
            "index 60 has already been scraped\n",
            "index 61 has already been scraped\n",
            "index 62 has already been scraped\n",
            "index 63 has already been scraped\n",
            "index 64 has already been scraped\n",
            "index 65 has already been scraped\n",
            "index 66 has already been scraped\n",
            "index 67 has already been scraped\n",
            "index 68 has already been scraped\n",
            "index 69 has already been scraped\n",
            "index 70 has already been scraped\n",
            "index 71 has already been scraped\n",
            "index 72 has already been scraped\n",
            "index 73 has already been scraped\n",
            "index 74 has already been scraped\n",
            "index 75 has already been scraped\n",
            "index 76 has already been scraped\n",
            "index 77 has already been scraped\n",
            "index 78 has already been scraped\n",
            "index 79 has already been scraped\n",
            "index 80 has already been scraped\n",
            "index 81 has already been scraped\n",
            "index 82 has already been scraped\n",
            "index 83 has already been scraped\n",
            "index 84 has already been scraped\n",
            "index 85 has already been scraped\n",
            "index 86 has already been scraped\n",
            "index 87 has already been scraped\n",
            "index 88 has already been scraped\n",
            "index 89 has already been scraped\n",
            "index 90 has already been scraped\n",
            "index 91 has already been scraped\n",
            "index 92 has already been scraped\n",
            "index 93 has already been scraped\n",
            "index 94 has already been scraped\n",
            "index 95 has already been scraped\n",
            "index 96 has already been scraped\n",
            "index 97 has already been scraped\n",
            "index 98 has already been scraped\n",
            "index 99 has already been scraped\n",
            "index 100 has already been scraped\n",
            "index 101 has already been scraped\n",
            "index 102 has already been scraped\n",
            "index 103 has already been scraped\n",
            "index 104 has already been scraped\n",
            "index 105 has already been scraped\n",
            "index 106 has already been scraped\n",
            "index 107 has already been scraped\n",
            "index 108 has already been scraped\n",
            "index 109 has already been scraped\n",
            "index 110 has already been scraped\n",
            "index 111 has already been scraped\n",
            "index 112 has already been scraped\n",
            "index 113 has already been scraped\n",
            "index 114 has already been scraped\n",
            "index 115 has already been scraped\n",
            "index 116 has already been scraped\n",
            "index 117 has already been scraped\n",
            "index 118 has already been scraped\n",
            "index 119 has already been scraped\n",
            "index 120 has already been scraped\n",
            "index 121 has already been scraped\n",
            "index 122 has already been scraped\n",
            "index 123 has already been scraped\n",
            "index 124 has already been scraped\n",
            "index 125 has already been scraped\n",
            "index 126 has already been scraped\n",
            "index 127 has already been scraped\n",
            "index 128 has already been scraped\n",
            "index 129 has already been scraped\n",
            "index 130 has already been scraped\n",
            "index 131 has already been scraped\n",
            "index 132 has already been scraped\n",
            "index 133 has already been scraped\n",
            "index 134 has already been scraped\n",
            "index 135 has already been scraped\n",
            "index 136 has already been scraped\n",
            "index 137 has already been scraped\n",
            "index 138 has already been scraped\n",
            "index 139 has already been scraped\n",
            "index 140 has already been scraped\n",
            "index 141 has already been scraped\n",
            "index 142 has already been scraped\n",
            "index 143 has already been scraped\n",
            "index 144 has already been scraped\n",
            "index 145 has already been scraped\n",
            "index 146 has already been scraped\n",
            "index 147 has already been scraped\n",
            "index 148 has already been scraped\n",
            "index 149 has already been scraped\n",
            "index 150 has already been scraped\n",
            "index 151 has already been scraped\n",
            "index 152 has already been scraped\n",
            "index 153 has already been scraped\n",
            "index 154 has already been scraped\n",
            "index 155 has already been scraped\n",
            "index 156 has already been scraped\n",
            "index 157 has already been scraped\n",
            "index 158 has already been scraped\n",
            "index 159 has already been scraped\n",
            "scraping text from index 160\n",
            "scraping text from index 161\n",
            "scraping text from index 162\n",
            "scraping text from index 163\n",
            "scraping text from index 164\n",
            "scraping text from index 165\n",
            "scraping text from index 166\n",
            "scraping text from index 167\n",
            "scraping text from index 168\n",
            "scraping text from index 169\n",
            "scraping text from index 170\n",
            "scraping text from index 171\n",
            "scraping text from index 172\n",
            "scraping text from index 173\n",
            "scraping text from index 174\n",
            "scraping text from index 175\n",
            "scraping text from index 176\n",
            "scraping text from index 177\n",
            "scraping text from index 178\n",
            "scraping text from index 179\n",
            "scraping text from index 180\n",
            "scraping text from index 181\n",
            "scraping text from index 182\n",
            "scraping text from index 183\n",
            "scraping text from index 184\n",
            "scraping text from index 185\n",
            "scraping text from index 186\n",
            "scraping text from index 187\n",
            "scraping text from index 188\n",
            "scraping text from index 189\n",
            "scraping text from index 190\n",
            "scraping text from index 191\n",
            "scraping text from index 192\n",
            "scraping text from index 193\n",
            "scraping text from index 194\n",
            "scraping text from index 195\n",
            "scraping text from index 196\n",
            "scraping text from index 197\n",
            "scraping text from index 198\n",
            "scraping text from index 199\n",
            "scraping text from index 200\n",
            "scraping text from index 201\n",
            "scraping text from index 202\n",
            "scraping text from index 203\n",
            "scraping text from index 204\n",
            "scraping text from index 205\n",
            "scraping text from index 206\n",
            "scraping text from index 207\n",
            "scraping text from index 208\n",
            "scraping text from index 209\n",
            "scraping text from index 210\n",
            "scraping text from index 211\n",
            "scraping text from index 212\n",
            "scraping text from index 213\n",
            "scraping text from index 214\n",
            "scraping text from index 215\n",
            "scraping text from index 216\n",
            "scraping text from index 217\n",
            "scraping text from index 218\n",
            "scraping text from index 219\n",
            "scraping text from index 220\n",
            "scraping text from index 221\n",
            "scraping text from index 222\n",
            "scraping text from index 223\n",
            "scraping text from index 224\n",
            "scraping text from index 225\n",
            "scraping text from index 226\n",
            "scraping text from index 227\n",
            "scraping text from index 228\n",
            "scraping text from index 229\n",
            "scraping text from index 230\n",
            "scraping text from index 231\n",
            "scraping text from index 232\n",
            "scraping text from index 233\n",
            "scraping text from index 234\n",
            "scraping text from index 235\n",
            "scraping text from index 236\n",
            "scraping text from index 237\n",
            "scraping text from index 238\n",
            "scraping text from index 239\n",
            "scraping text from index 240\n",
            "scraping text from index 241\n",
            "scraping text from index 242\n",
            "scraping text from index 243\n",
            "scraping text from index 244\n",
            "scraping text from index 245\n",
            "scraping text from index 246\n",
            "scraping text from index 247\n",
            "scraping text from index 248\n",
            "scraping text from index 249\n",
            "scraping text from index 250\n",
            "scraping text from index 251\n",
            "scraping text from index 252\n",
            "scraping text from index 253\n",
            "scraping text from index 254\n",
            "scraping text from index 255\n",
            "scraping text from index 256\n",
            "scraping text from index 257\n",
            "scraping text from index 258\n",
            "scraping text from index 259\n",
            "scraping text from index 260\n",
            "scraping text from index 261\n",
            "scraping text from index 262\n",
            "scraping text from index 263\n",
            "scraping text from index 264\n",
            "scraping text from index 265\n",
            "scraping text from index 266\n",
            "scraping text from index 267\n",
            "scraping text from index 268\n",
            "scraping text from index 269\n",
            "scraping text from index 270\n",
            "scraping text from index 271\n",
            "scraping text from index 272\n",
            "scraping text from index 273\n",
            "scraping text from index 274\n",
            "scraping text from index 275\n",
            "scraping text from index 276\n",
            "scraping text from index 277\n",
            "scraping text from index 278\n",
            "scraping text from index 279\n",
            "scraping text from index 280\n",
            "scraping text from index 281\n",
            "scraping text from index 282\n",
            "scraping text from index 283\n",
            "scraping text from index 284\n",
            "scraping text from index 285\n",
            "scraping text from index 286\n",
            "scraping text from index 287\n",
            "scraping text from index 288\n",
            "scraping text from index 289\n",
            "scraping text from index 290\n",
            "scraping text from index 291\n",
            "scraping text from index 292\n",
            "scraping text from index 293\n",
            "scraping text from index 294\n",
            "scraping text from index 295\n",
            "scraping text from index 296\n",
            "scraping text from index 297\n",
            "scraping text from index 298\n",
            "scraping text from index 299\n",
            "scraping text from index 300\n",
            "scraping text from index 301\n",
            "scraping text from index 302\n",
            "scraping text from index 303\n",
            "scraping text from index 304\n",
            "scraping text from index 305\n",
            "scraping text from index 306\n",
            "scraping text from index 307\n",
            "scraping text from index 308\n",
            "scraping text from index 309\n",
            "scraping text from index 310\n",
            "scraping text from index 311\n",
            "scraping text from index 312\n",
            "scraping text from index 313\n",
            "scraping text from index 314\n",
            "scraping text from index 315\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-285298231726>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfill_in_missing_time_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnds_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munscraped_programs_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscripts_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_option\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-e1850311cd52>\u001b[0m in \u001b[0;36mnds_scrape\u001b[0;34m(programs_file, unscraped_programs_file, transcripts_dir, driver_option)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scraping text from index {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;31m# go to link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;31m# # extract text from the body of the website\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/html/body/table/tbody\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;34m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self._url}{path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             return self.request_encode_body(\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "fill_in_missing_time_title(urls_file)\n",
        "nds_scrape(urls_file, unscraped_programs_file, transcripts_dir, driver_option=op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhmbhWipZMia"
      },
      "source": [
        "# NEXT TO FIX/CONSIDER (NOT ranked by priority):\n",
        "## 1 - Local stations: now have \"AM/PM\" options in SOURCES box: eg. \"WCCO\" vs \"WCCO-AM\"\n",
        "## 2 - Find a way to avoid the need of separate rows for different dates in programs.csv\n",
        "## 3 - National stations: \"National News Network\" vs \"National News Network (All Other)\" in STATES/CITIES box\n",
        "## 4 - ~Find a way to NOT overwrite programs_file when we add more programs from new queries~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVQecFbm6DRY"
      },
      "outputs": [],
      "source": [
        "if pathlib.Path(urls_file).is_file():\n",
        "      df_urls = pd.read_csv(urls_file)\n",
        "      done_Query_List = df_urls.loc[:, ['Date', 'Source', 'Market']].drop_duplicates().reset_index(drop=True)\n",
        "      print(done_Query_List)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFfc136q-nla"
      },
      "outputs": [],
      "source": [
        "Programs_List = pd.read_csv(programs_file)\n",
        "# The three columns identify exactly what is needed to make a query in NDS\n",
        "Query_List = Programs_List.loc[:, ['Date', 'Source', 'Market']].drop_duplicates().reset_index(drop=True)\n",
        "print(Query_List)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_332c4Z-0sI"
      },
      "outputs": [],
      "source": [
        "d3 = pd.concat([Query_List, done_Query_List]).drop_duplicates(keep=False)\n",
        "print(d3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu4NzA3y-7i-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(programs_file)\n",
        "df['Scraped'] = False\n",
        "df_sorted = df.sort_values(['Market', 'Source', 'Date'])\n",
        "df_sorted.reset_index(drop=True, inplace=True)\n",
        "df_sorted.to_csv('./programs.csv', index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXYNoDTOPRwZ"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame([['2021-04-20', 'ABC', 'National News Networks'], ['2021-04-20', 'ABC', 'National Networks (All Other)']], columns=['Date', 'Source', 'Market'])\n",
        "df['Scraped'] = False\n",
        "df_sorted = df.sort_values(['Market', 'Source', 'Date'])\n",
        "df_sorted.reset_index(drop=True, inplace=True)\n",
        "df_sorted.to_csv('./programs.csv', index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLW3ftxXCzEj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr4zwF0KBt5I"
      },
      "source": [
        "\n",
        "**Errors found as of Jun 3 2022:**   \n",
        "```\n",
        "delete date complete\n",
        "Message: element not interactable\n",
        "```\n",
        "*   \n",
        "```\n",
        "select city complete\n",
        "Message: element click intercepted: Element <div class=\"k-multiselect-wrap k-floatwrap\" deselectable=\"on\">...</div> is not clickable at point (120, 526). Other element would receive the click: <div class=\"js-spin-overlay\" style=\"position: absolute; opacity: 0.5; z-index: 10009; background-color: rgb(0, 0, 0); left: 0px; top: 0px; width: 785px; height: 600px;\"></div>\n",
        "```\n",
        "*\n",
        "```\n",
        "Message: element click intercepted: Element <a href=\"/ProgramList\">...</a> is not clickable at point (236, 63). Other element would receive the click: <div class=\"js-spin-overlay\" style=\"position: absolute; opacity: 0.5; z-index: 10009; background-color: rgb(0, 0, 0); left: 0px; top: 0px; width: 785px; height: 600px;\"></div>\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNKMb4eQHzEq"
      },
      "outputs": [],
      "source": [
        "nds_crawl(username, password, programs_file, urls_file, failed_query_file, driver_option=op)\n",
        "fill_in_missing_time_title(urls_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQVKsuRKH79r"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(urls_file)\n",
        "df2 = pd.read_csv(failed_query_file)\n",
        "df2\n",
        "# df = df.loc[:, ['Date', 'Time', 'Title', 'Source', 'URL']].drop_duplicates().reset_index(drop=True)\n",
        "# df.shape[0]\n",
        "# df\n",
        "df2 = pd.read_csv(urls_file)\n",
        "df2.sort_values(['Date', 'Time', 'Title', 'Market'], inplace=True, ignore_index=True)\n",
        "df2.shape[0]\n",
        "df2.to_csv('dup.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}